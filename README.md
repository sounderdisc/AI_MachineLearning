# AI & Machine Learning Showcase
In this repo, there are Classifiers for the classic MINST and CIFAR-10 datasets, and a Reinforcement Learning agent for OpenAI Gym's Lunar lander, and an indie game titled [Super Hexagon.](https://en.wikipedia.org/wiki/Super_Hexagon)


## Running the Code
I have included information about how the models performed for all these tasks in their respective folders, and a [video demonstration of the code running is available here,](https://www.youtube.com/watch?v=paL2-yqP_Lk&ab_channel=sounderdiscISW) but in case you wish to run the code yourself anyway, I have included a .yml environment file for Anaconda. In case you need guidance utilizing that, check [this link.](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) If you intend to run this code, you will want a computer with a dedicated graphics card and [Cuda](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exe_local) installed. You should be able to run the Classifiers and the Reinforcement Learning agent on the Lunar Lander example off of only a cpu, but it will be very slow. If you wish to run the Reinforcement Learning agent on Super Hexagon, then you will need to own a copy of the game and will absolutely need a dedicated graphics card with Cuda because the agent gives inputs to the game runs which runs in real time.


## MINST and CIFAR-10 Classifiers
These are relatively mundane classifiers for the MINST and CIFAR-10 datasets. These datasets contain labeled images of handwritten digits and pictures of ten different classes respectively. My implementation of a MINST Classifier uses a few fully connected layers hooked up directly to each and every pixel of the input image, and my implementation of a CIFAR-10 image Classifier uses a Convolutional Neural Network followed by fully connected layers. See the results.txt file for details on their performances.


## The Reinforcement Learning Agent
The same agent class is used for both the openAI Gym Lunar Lander game and Super Hexagon. It is similar to what is described in the section titled ["Double Q Learning" in this research paper.](https://arxiv.org/pdf/1710.02298.pdf) The constructor of the class allows the caller to create an object that takes in a custom input shape and a custom number of output actions. It can also optionally use a series of Convolutional Layers for the input before going to several fully connected layers. The instance used for the Lunar Lander game does not use the CNN layers while the instance used for Super Hexagon does. Each game step, the agent stores memories about the game state, its action, and the subsequent game state including the reward received and if the game ended. It has two neural networks, one for predicting the best action for a given game state, and another to predict the expected reward from the subsequent game state. Both of these values are used in the agent's loss function.


## Lunar Lander
My implementation of the agent was able to solve this game in 800 games of training. Interestingly, the agent suffered from catastrophic forgetting after this, and the model that stopped training after 800 games performed much better than models that trained for 1,000 and 5,000 games. It wasn't until 10,000 games that the model started performing better than it did after playing 800 games. I used pyplot to graph data recorded during training and testing, so look in the PremadeModelsAndGraphs folder if you are interested in more detailed data. I also saved several trained models in the same directory if you wish to work with them yourself.

One thing that I attempted to try and combat catastrophic forgetting that I did not find any research papers is what I call "Strong Arm Remembering" where I save the pair of neural networks that give the best score and revert to those networks any time the model scores significantly lower than its best score. I ended up doing this only for the last 10% of training games played, and I found it to have mixed results. It seems like if this technique is useful at all, its to squeeze the last little bit of improvement you can out of an already trained and competent agent. The data in this chart is the same high scores you can find in the PremadeModelsAndGraphs folder and compares the test performance of the "Best" model as obtained by Strong Arm Remembering with the "Last" model as obtained by normal training for various numbers of games of training.

![image](https://user-images.githubusercontent.com/42709150/147046187-bc87b6f5-a929-4558-af2e-0b7fcd4b4499.png)


## Super Hexagon
Unlike games in openAI Gym, I have to set up getting game data in and action output out on my own. I decided to use PIL to take screenshots of a certain location of the screen while Super Hexagon played in windowed mode. I initially intended to run the game fullscreen, capture the entire screen, and then shrink the image, but there was some other software that prevented me from capturing new frames from the game's window while in fullscreen. While I don't know what it was, I suspect the culprit was either Nvidia GeForce, Steam, or Windows. For keyboard output to the game, I used the library PyDirectInput, because PyAutoGui sends virtual key codes which does not work with many directX games.

One thing to note is that the only possible reward signal without using prior knowledge of the environment is the game timer. If I were to provide the AI a reward for dodging obstacles or for staying on one of the safe faces of the center hexagon, then I would need to have a way to know where the obstacles are, and if I do that, then a solution to the game that doesnt involve a neural network would make more sense. One thing that comes to mind is using classic computer vision to find the corners of obstacles and hard coding a script to move to the face of the center hexagon with the greatest distance to the nearest obstacle. [I have done such classic computer vision before in one of my classes](https://colab.research.google.com/drive/19t5WWMYpHSfFju87RXxCjVtPkfozqe5A?usp=sharing), and I didn't think I would learn anything new from going with such an approach.

At first I allowed the AI to take in screenshots and output actions as fast as my computer could manage. I also limited the AI to two actions, tapping left or right very briefly, but after observation and research, I saw that doing so unnecessarily increased the number of time steps between an action and its reward which hampers learning.

I ended up limiting the frame sampling and input rate to 4 frames a second with 29 action outputs, even though my computer could go faster, and the game only has 2 inputs. The extra inputs were choosing not to put in any input and holding down the left and right arrow keys for variable amounts of time. I did this because the network only has the game timer as a reward structure. As a result, the only useful memories the AI can form is of life-and-death decisions on which input to take because the network only looks forward one frame when learning. If I were to increase the framerate of the agent, then it would have much fewer opportunities to form valuable memories.

Ultimately, the agent is not learning significantly over the course of 3 or 4 hours. One thing to keep in mind is that in [Deep Mind's Rainbow paper](https://arxiv.org/pdf/1710.02298.pdf) the figure shows that it takes about *18 million frames* to surpass human performance. At 60 frames a second, that amounts to about *80 hours of gameplay,* and this doesn't even account for time required to train the model outside of the game itself running. Additionally, that paper used an emulator for the Atari games their models were learning from which could possibly allow for pausing and stepping through the game frame by frame in a controlled time frame, whereas I am running the game in real time and capturing the screen. The Atari also has a much smaller color palette and simpler art style, the games do not spin and flash like a disco ball, and the Atari has a smaller input image than my screen capture. Considering that I am currently a single undergraduate student running 5 year old hardware on my personal gaming rig, they likely had better hardware than I do, even though this paper is almost 5 years old. This means if it took them *at least* 80 hours on better hardware and simpler games, I don't expect to be able to dedicate whatever amount of time and computational resources are required to determine if my agent will never learn, or if he's just a late bloomer.

Even though this is likely the end of the road for this project due to hardware constraints, I learned a lot about PyTorch, Anaconda environments, reinforcement learning, classification problems in machine learning, screen capture in python, and overall neural networks.

